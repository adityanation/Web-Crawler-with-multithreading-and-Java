# ğŸ•·ï¸ Multithreaded Web Crawler in Java

This project is a simple yet powerful **Multithreaded Web Crawler** built using **Java**. It efficiently crawls web pages starting from a seed URL, extracts links, and explores them concurrently using Java's multithreading capabilities.

## ğŸš€ Features

- ğŸŒ Crawl web pages starting from a seed URL
- ğŸ”— Extract and follow internal/external hyperlinks
- ğŸ§µ Uses Java's `ExecutorService` for multithreading
- ğŸ”’ Thread-safe visited URL tracking
- â›” URL depth and domain constraints (configurable)
- ğŸ“ Simple HTML parsing using `JSoup`

## ğŸ“ Project Structure

```
web-crawler-java/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Main.java               # Entry point of the application
â”‚   â”œâ”€â”€ WebCrawler.java         # Crawler logic with multithreading
â”‚   â””â”€â”€ LinkExtractor.java      # Utility to extract links from HTML
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ pom.xml (if using Maven)
â””â”€â”€ build.gradle (if using Gradle)
```

## ğŸ› ï¸ Requirements

- Java 8 or higher
- Maven or Gradle (optional, for dependency management)
- Internet connection (for crawling!)

## ğŸ“¦ Dependencies

This crawler uses the [JSoup](https://jsoup.org/) library to parse and extract HTML content.

Add to `pom.xml` (Maven):

```xml
<dependency>
  <groupId>org.jsoup</groupId>
  <artifactId>jsoup</artifactId>
  <version>1.15.3</version>
</dependency>
```

Or to `build.gradle` (Gradle):

```groovy
implementation 'org.jsoup:jsoup:1.15.3'
```

## ğŸ”§ How to Use

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/web-crawler-java.git
cd web-crawler-java
```

### 2. Compile and Run

Using Maven:

```bash
mvn compile
mvn exec:java -Dexec.mainClass="Main"
```

Or directly using `javac` and `java`:

```bash
javac -d bin src/*.java
java -cp bin Main
```

### 3. Example Output

```
Starting crawl from: https://example.com
[Thread-1] Crawling: https://example.com
[Thread-2] Crawling: https://example.com/about
[Thread-3] Crawling: https://example.com/contact
...
Crawl complete. Total URLs visited: 100
```

## âš™ï¸ Configuration

You can configure the crawler with:

- `MAX_DEPTH`: How deep the crawler should go
- `MAX_THREADS`: Number of concurrent threads
- `DOMAIN_RESTRICTION`: Limit crawl to a specific domain

These can be set in the `WebCrawler.java` file or passed via command line depending on implementation.

## ğŸ§  Concepts Used

- Java `ExecutorService` for managing threads
- Thread-safe data structures (`ConcurrentHashMap`, `BlockingQueue`)
- HTML parsing with JSoup
- URL normalization and domain filtering

## ğŸ§ª Future Improvements

- Save crawled data to disk or database
- Add politeness delay (to avoid hammering servers)
- Respect `robots.txt`
- Better error handling and retries

## ğŸ“„ License

This project is open-source under the [MIT License](LICENSE).
